{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab02_solved.ipynb","provenance":[{"file_id":"1pRK1zCbZ8mpqWixDsu1RDw38STVKxQhG","timestamp":1647933972605},{"file_id":"1tzBby0OxYWC9cbWcNunpR9yHMJC3hZ_t","timestamp":1616023677946}],"collapsed_sections":[],"authorship_tag":"ABX9TyNbnE65mrLftm/8PBb3fKzU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4pu6YKs3JVHG"},"source":["# Lab2"]},{"cell_type":"markdown","metadata":{"id":"hzloHzwEJbOZ"},"source":["In this lab, we are going to learn about NLTK, Spacy and some visualization techniques, let's begin."]},{"cell_type":"markdown","metadata":{"id":"LkFy8w8LJW5i"},"source":["# NLTK\n","\n","NLTK is one of the major NLP packages in Python. It is targeted at learners rather than being a production library which makes a good starting point for our purposes.\n","\n","The first step is to import nltk then make sure that all the necessary files are downloaded."]},{"cell_type":"code","metadata":{"id":"ot1hvTcxVAQL"},"source":["import nltk\n","nltk.download(\"all\", quiet=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jUYVs5ICVAt1"},"source":["If you are running on your personal machine (not within Google colab), you only need to do this once. \n","\n","NLTK offers a special module called \"book\" which can be imported using ```from nltk.book import *``` \n","\n","After printing a welcome message, it loads the text of several books (this will take a few seconds)."]},{"cell_type":"code","metadata":{"id":"HLQtW_CLVmCg"},"source":["from nltk.book import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tquXN7iJV16H"},"source":["This module contains a number of Corpora that are ready to practice on. The Corpora are named: *text1* - *text9*"]},{"cell_type":"code","metadata":{"id":"QKG-wIKlV-Aq"},"source":["text1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TE6XRdkKWHcf"},"source":["text2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tr0CMsyZV-Np"},"source":["##  Searching Text\n","There are many ways to examine the context of a text apart from simply reading it. **A concordance view shows us every occurrence of a given word, together with some context**."]},{"cell_type":"code","metadata":{"id":"eaDZGlTvWXWG"},"source":["text1.concordance(\"true\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qJblppzExJPn"},"source":["help(nltk.Text.concordance)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6eYpFKg32wv1"},"source":["Try it yourself. Let's find more about the words: [\"often\", \"test\", \"extreme\"] is text2"]},{"cell_type":"code","metadata":{"id":"YdqpzSCM2vwy"},"source":["# Your code goes here\n","text2.concordance(\"often\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text2.concordance(\"test\")"],"metadata":{"id":"NbN_OoE6i_vp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text2.concordance(\"extreme\")"],"metadata":{"id":"hGWpKGTAjDFI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sttxNYIXWXiu"},"source":["A concordance permits us to see words in context. For example, we saw that monstrous occurred in contexts such as the \\_\\_\\_ pictures and a \\_\\_\\_ size . \n","\n","What other words appear in a similar range of contexts? We can find out by appending the term similar to the name of the text in question, then inserting the relevant word in parentheses:"]},{"cell_type":"code","metadata":{"id":"2eZZBBiuW7CF"},"source":["text1.similar(\"monstrous\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xrIWgQZFXCK6"},"source":["text2.similar(\"monstrous\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A7LpxuTDW7I4"},"source":["Observe that we get different results for different texts. Austen uses this word quite differently from Melville; for her, monstrous has positive connotations, and sometimes functions as an intensifier like the word very.\n","\n","The method ```common_contexts``` allows us to examine just the contexts that are shared by two or more words, such as monstrous and very. "]},{"cell_type":"code","metadata":{"id":"7ttTzdVpXQqh"},"source":["text1.common_contexts([\"monstrous\", \"true\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MPZbNAHCXQBw"},"source":["Your Turn: pick another pair of words and compare their usage in two different texts, using the ```similar()``` and ```common_contexts()``` functions.\n"]},{"cell_type":"code","metadata":{"id":"w3KW8t5D425d"},"source":["text1.similar(\"benevolent\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-WzGnsq05kG"},"source":["# Your code goes here\n","\n","# find the common contexts in which the words: great and dangerous are used in text1\n","text1.common_contexts([\"great\", \"dangerous\"])\n","\n","# find the common contexts in which the words: benevolent and fish are used in text1\n","text1.common_contexts([\"benevolent\", \"fish\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gVihEk6L060k"},"source":["It is one thing to automatically detect that a particular word occurs in a text, and to display some words that appear in the same context. \n","\n","However, we can also determine the location of a word in the text: how many words from the beginning it appears. This positional information can be displayed using a dispersion plot. Each stripe represents an instance of a word, and each row represents the entire text."]},{"cell_type":"code","metadata":{"id":"RBMOFFsJXcOR"},"source":["text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KIwOZLyi6NqR"},"source":["Try more words (e.g., liberty, constitution), and different texts. "]},{"cell_type":"code","metadata":{"id":"h42HlzRJ6RWg"},"source":["# Your code goes here\n","text4.dispersion_plot([\"liberty\", \"nation\", \"constitution\", \"kindness\", \"pope\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nn0QmfsrXcV0"},"source":["\n","For these plots to work, it is assumed that ```numpy``` and ```matplotlib``` are installed."]},{"cell_type":"markdown","metadata":{"id":"P4Ac1p0lXuyY"},"source":["\n","\n","##   Counting Vocabulary\n","The most obvious fact about texts that emerges from the preceding examples is that they differ in the vocabulary they use. \n","\n","Let's begin by finding out the length of a text from start to finish, in terms of the words and punctuation symbols that appear. \n","\n","Let's find the number of words in the book of Genensis:"]},{"cell_type":"code","metadata":{"id":"lYg6xoflYZ4B"},"source":["len(text3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tse16z5EYZI8"},"source":["So Genesis has 44,764 words and punctuation symbols, or \"tokens.\" \n","\n","This includes duplicate tokens. To find the number of unique words, we use the ```set``` data structure as follows:"]},{"cell_type":"code","metadata":{"id":"dZUY_xEwYqtz"},"source":["print(set(text3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FhHUEUJG7tCn"},"source":["and to get a sorted set, we use the Python ```sorted()``` function:"]},{"cell_type":"code","metadata":{"id":"sAFlnxij71qE"},"source":["print(sorted(set(text3)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T3xn-ufTZB90"},"source":["len(set(text3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bWzz1HgGYq1u"},"source":["Although it has 44,764 tokens, this book has only 2,789 distinct words, or \"word types.\" \n","\n","Now, let's calculate a measure of the lexical richness of the text. The next example shows us that the number of distinct words is just 6% of the total number of words, or equivalently that each word is used 16 times on average\n"]},{"cell_type":"code","metadata":{"id":"I7qEndcrZSWk"},"source":["len(set(text3)) / len(text3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Q2UWXUw0SUT"},"source":["len(text3)/len(set(text3)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wFIhQ-oaZSd6"},"source":["Next, let's focus on particular words. We can count how often a word occurs in a text, and compute what percentage of the text is taken up by a specific word:"]},{"cell_type":"code","metadata":{"id":"QuDVfwpWZhMI"},"source":["text3.count(\"smote\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vfiqeKNPZoBc"},"source":["100 * text4.count(\"a\") / len(text4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gRNw7RetZghy"},"source":["Your Turn: How many times does the word *lol* appear in text5? How much is this as a percentage of the total number of words in this text?\n"]},{"cell_type":"code","metadata":{"id":"jxhyP6uVZv0u"},"source":["# Your code goes here\n","100 * text5.count(\"lol\") / len(text5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cdTUB9vdaZFh"},"source":["To simplify things, let's create a functions for calculating lexical diversity"]},{"cell_type":"code","metadata":{"id":"8B9ybSUbaLwF"},"source":["def lexical_diversity(text):\n","  return len(set(text)) / len(text)\n","\n","def percentage(count, total):\n","  return 100 * count / total"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sSmjI5wfa023"},"source":["We can go ahead and use these functions, let's find the number of tokens, the number of types, the lexical diversity for for the for the following:"]},{"cell_type":"code","metadata":{"id":"9rI2rb9Xbl4n"},"source":["from nltk.corpus import brown\n","\n","print(f\"The type of brown is: {type(brown)}\")\n","\n","len(brown.words()), len(set(brown.words())), lexical_diversity(brown.words())"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["brown.paras()[:10]"],"metadata":{"id":"9SOYWtB2isvP"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUlv45R6eHDj"},"source":["print(brown.raw()[:10])\n","print(brown.words()[:10])\n","print(brown.sents()[:2])\n","print(brown.paras()[:2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r9l8nU0eZwH2"},"source":["\n","\n","##  Computing with Language: Simple Statistics\n","Now, we pick up the question of what makes a text distinct, and use automatic methods to find characteristic words and expressions of a text."]},{"cell_type":"markdown","metadata":{"id":"HkzW0Wh9fNRM"},"source":["### Frequency Distributions\n","How can we automatically identify the words of a text that are most informative about the topic and genre of the text? \n","\n","Imagine how you might go about finding the 50 most frequent words of a book.\n","\n","Since we often need frequency distributions in language processing, NLTK provides built-in support for them. Let's use a ```FreqDist()``` to find the 50 most frequent words of Moby Dick:"]},{"cell_type":"code","metadata":{"id":"b9FSqN-HhNZL"},"source":["fdist1 = FreqDist(text1) \n","print(fdist1) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EqNK27v-hQU0"},"source":["fdist1.most_common(50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PUqv8nr9-Zrf"},"source":["To find the frequency of a particular word, you can can access it using the word string as index, the output of FreqDist is a dictionary."]},{"cell_type":"code","metadata":{"id":"3q9XNu0LhVVw"},"source":["fdist1['whale']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZXyqHN0hwX1"},"source":["Your Turn: What are the top 20 tokens in text2. "]},{"cell_type":"code","metadata":{"id":"PEXFhYl_iAxH"},"source":["# Your code goes here\n","fdist2 = FreqDist(text2)\n","fdist2.most_common(20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lhhLY9rp-vYH"},"source":["Remember, we have calculated the frequencies in Lab 1 in pure Python."]},{"cell_type":"markdown","metadata":{"id":"roHngrruh-q1"},"source":["Let's return to our task of finding words that characterize a text. \n","\n","Notice that the long words in *text4* reflect its national focus — constitutionally, transcontinental — whereas those in text5 reflect its informal content: boooooooooooglyyyyyy and yuuuuuuuuuuuummmmmmmmmmmm. \n","\n","Have we succeeded in automatically extracting words that typify a text? Well, these very long words are often hapaxes (i.e., unique) and perhaps it would be better to find frequently occurring long words. \n","\n","This seems promising since it eliminates frequent short words (e.g., the) and infrequent long words (e.g. antiphilosophists). \n","\n","Here are all words from the chat corpus that are longer than seven characters, that occur more than seven times:\n","\n"," "]},{"cell_type":"code","metadata":{"id":"8ZHgV0oojPR6"},"source":["fdist5 = FreqDist(text5)\n","sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mb7zYrIqjQB4"},"source":["###   Counting Other Things\n","Counting words is useful, but we can count other things too. For example, we can look at the distribution of word lengths in a text, by creating a ```FreqDist``` out of a long list of numbers, where each number is the length of the corresponding word in the text:\n","\n"]},{"cell_type":"code","metadata":{"id":"KWv7VRzNjbgJ"},"source":["[len(w) for w in text1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRkt4uO5jemD"},"source":["fdist = FreqDist(len(w) for w in text1)\n","fdist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMDqZkk45l3H"},"source":["[w for w in text1 if len(w) == 20]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aeXzRBNGjbmh"},"source":["The result is a distribution containing a quarter of a million items, each of which is a number corresponding to a word token in the text. But there are at most only 20 distinct items being counted, the numbers 1 through 20, because there are only 20 different word lengths. I.e., there are words consisting of just one character, two characters, ..., twenty characters, but none with twenty one or more characters. \n","\n","\n","One might wonder how frequent the different lengths of word are (e.g., how many words of length four appear in the text, are there more words of length five than length four, etc). We can do this as follows:\n"]},{"cell_type":"code","metadata":{"id":"hdGQEJ93jt9O"},"source":["fdist.most_common(n=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bh7iS5pzjuEx"},"source":["fdist.max()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DZMPLEcUj3D5"},"source":["fdist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xbx6aM4IjuNY"},"source":["fdist.freq(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"53UO61kejuqu"},"source":["From this we see that the most frequent word length is 3, and that words of length 3 account for roughly 50,000 (or 20%) of the words making up the book. \n","\n","\n","Although we will not pursue it here, further analysis of word length might help us understand differences between authors, genres, or languages.\n","\n","### Functions Defined for NLTK's Frequency Distributions\n","\n","Example |\tDescription\n","--- | ---\n","`fdist = FreqDist(samples)` |\tcreate a frequency distribution containing the given samples\n","`fdist[sample] += 1`\t| increment the count for this sample\n","`fdist['monstrous']`\t| count of the number of times a given sample occurred\n","`fdist.freq('monstrous')`\t| frequency of a given sample\n","`fdist.N()`\t| total number of samples\n","`fdist.most_common(n)`\t| the n most common samples and their frequencies\n","`for sample in fdist:`\t| iterate over the samples\n","`fdist.max()`\t| sample with the greatest count\n","`fdist.tabulate()`\t| tabulate the frequency distribution\n","`fdist.plot()`\t| graphical plot of the frequency distribution\n","`fdist.plot(cumulative=True)`\t| cumulative plot of the frequency distribution\n","`fdist1 \\|= fdist2`\t| update fdist1 with counts from fdist2\n","`fdist1 < fdist2`\t| test if samples in fdist1 occur less frequently than in fdist2\n"]},{"cell_type":"markdown","metadata":{"id":"EP4w0KU0koAb"},"source":["###   Exercise 1\n","\n","a. How many words are there in text2? How many distinct words are there?"]},{"cell_type":"code","metadata":{"id":"bv38eBjJkoMY"},"source":["# Your code goes here\n","print(\"There are {} words in text2\".format(len(text2)))\n","print(\"There are {} distinct words in text2\".format(len(set(text2))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x2KPIzh6k4Pa"},"source":["b. Compare the lexical diversity scores for humour (text6) and romance fiction (text2). Which genre is more lexically diverse?"]},{"cell_type":"code","metadata":{"id":"rQ7cPaR4lBWf"},"source":["# Your code goes here\n","print(\"The lexical diversity of text6 (humour) is {}\".format(lexical_diversity(text6)))\n","print(\"The lexical diversity of text2 (romance) is {}\".format(lexical_diversity(text2)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z2x9VjyflEOk"},"source":["c. Produce a dispersion plot of the four main protagonists in Sense and Sensibility: Elinor, Marianne, Edward, and Willoughby. What can you observe about the different roles played by the males and females in this novel? Can you identify the couples?"]},{"cell_type":"code","metadata":{"id":"vTBH5MbVlN_f"},"source":["# Your code goes here\n","text2.dispersion_plot([\"Elinor\", \"Edward\", \"Marianne\", \"Willoughby\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jngeLZ3blPIv"},"source":["d. The first sentence of text3 is provided to you in the variable sent3. The index of *the* in sent3 is 1, because sent3[1] gives us 'the'. What are the indexes of the two other occurrences of this word in sent3?"]},{"cell_type":"code","metadata":{"id":"r9NdXtTJhvLZ"},"source":["# Your code goes here\n","print(sent3)\n","print([idx for idx, word in enumerate(sent3) if word == 'the'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wscx1RDhlubI"},"source":["e. Find all the four-letter words in the Chat Corpus (text5). With the help of a frequency distribution (FreqDist), show these words in decreasing order of frequency.\n","\n"]},{"cell_type":"code","metadata":{"id":"_ht2hQt31MoT"},"source":["# Your code goes here\n","import re\n","fd = FreqDist(w.lower() for w in text5 \n","              if len(w)==4 and re.match(r\"[a-zA-Z]{4}\", w))\n","fd.most_common()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"grtzZehc1NId"},"source":["f. Print all the uppercase words in Monty Python and the Holy Grail (text6)?"]},{"cell_type":"code","metadata":{"id":"VwNmCCfD1hP_"},"source":["# Your code goes here\n","[word for word in text6.tokens if word.isupper()]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Re8IH7NG1hpT"},"source":["g. Write expressions for finding all words in text6 that meet the conditions listed below. The result should be in the form of a list of words\n","\n","* Ending in ise\n","* Containing the letter z\n","* Containing the sequence of letters pt\n","* Having all lowercase letters except for an initial capital (i.e., titlecase)\n"]},{"cell_type":"code","metadata":{"id":"qs0LYav51yWI"},"source":["# Your code goes here\n","\n","# Ending in ise\n","print([word for word in text6.tokens if word[-3:] == 'ise'])\n","\n","# Containing the letter z\n","print([word for word in text6.tokens if 'z' in word])\n","\n","# Containing the sequence of letters pt\n","print([word for word in text6.tokens if 'pt' in word])\n","\n","# Having all lowercase letters except for an initial capital (i.e., titlecase)\n","print([word for word in text6.tokens if word.istitle()])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"osQr8oNSKqlz"},"source":["## Corpora"]},{"cell_type":"markdown","metadata":{"id":"IpzAfV6C1yve"},"source":["Practical work in Natural Language Processing typically uses large bodies of linguistic data, or corpora.\n","\n","### Accessing Text Corpora\n","A text corpus is a large body of text. Many corpora are designed to contain a careful balance of material in one or more genres. \n","\n","We look at various pre-defined texts that we accessed by typing ```from nltk.book import *```. However, since we want to be able to work with other texts, we now examine a variety of text corpora. \n","\n","NLTK provides access to a number of well known Corpora as well as allows us to build our own. We will examine the Corpora that are made availabe to us first."]},{"cell_type":"markdown","metadata":{"id":"0YI1yhqsEEDz"},"source":["**Gutenberg Corpus**\n","\n","NLTK includes a small selection of texts from the Project Gutenberg electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/. which can be accessed as follows"]},{"cell_type":"code","metadata":{"id":"sg5UIaYwEUE0"},"source":["from nltk.corpus import gutenberg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1n1aChZbEkSz"},"source":["we can find the list of ducuments included in this corpus as follows:"]},{"cell_type":"code","metadata":{"id":"erLZKZYFEsJR"},"source":["gutenberg.fileids()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMEAgM_tHwPo"},"source":["print(type(gutenberg))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xn9QnAtCF83O"},"source":["The Corpus is an instance of the class ```PlaintextCorpusReader```\n","\n","To be able to use the ```concordance``` method we saw earlier, we need to load the words as a ```Text``` object as follows:"]},{"cell_type":"code","metadata":{"id":"iLk96d4WF7jA"},"source":["emma = gutenberg.words(gutenberg.fileids()[0])\n","emma_text = nltk.Text(emma)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OsrZKu6eFoGF"},"source":["emma_text.concordance(\"dry\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GR6jH-KgGw78"},"source":["### Exercise 2\n","\n","Find the longest word in the file *chesterton-thursday.txt*"]},{"cell_type":"code","metadata":{"id":"p2lrTMlXG_fD"},"source":["chesterton_thursday = gutenberg.words('chesterton-thursday.txt')\n","longest = ''\n","for word in chesterton_thursday:\n","  if len(word) > len(longest):\n","    longest = word\n","longest"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-MWCxYnFHmlW"},"source":["NLTK offers a number of of Corpora, worth mentioning are the following:\n","\n","package name | Corpus name \n","--- | --- \n","webtext | Web Text \n","nps_chat | Instant messaging sessions \n","brown | Brown Corpus \n","reuters | Reuters Corpus \n","inaugural | Inaugural Address Corpus \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8h0iYe-QLXUG"},"source":["Summary of basic Corpus methods defined in NLTK\n","\n","Example\t| Description\n","--- | ---\n","`fileids()`\t| the files of the corpus\n","`fileids([categories])`\t| the files of the corpus corresponding to these categories \n","`categories()`\t| the categories of the corpus\n","`categories([fileids])`\t| the categories of the corpus corresponding to these files\n","`raw()`\t| the raw content of the corpus\n","`raw(fileids=[f1,f2,f3]) `|\tthe raw content of the specified files\n","`raw(categories=[c1,c2])` |\tthe raw content of the specified categories\n","`words()`\t| the words of the whole corpus\n","`words(fileids=[f1,f2,f3])` |\tthe words of the specified fileids\n","`words(categories=[c1,c2])`|\tthe words of the specified categories\n","`sents()`\t| the sentences of the whole corpus\n","`sents(fileids=[f1,f2,f3])` |\tthe sentences of the specified fileids\n","`sents(categories=[c1,c2])`\t| the sentences of the specified categories\n","`abspath(fileid)`\t| the location of the given file on disk\n","`encoding(fileid)`\t| the encoding of the file (if known)\n","`open(fileid)`\t| open a stream for reading the given corpus file\n","`root` |\tif the path to the root of locally installed corpus\n","`readme()`\t| the contents of the README file of the corpus"]},{"cell_type":"markdown","metadata":{"id":"pFBoDWYOJsU4"},"source":["NLTK's built in Corpora are good for learning the concepts, but eventually, we would like to operate on our own data. \n","\n","Text data in the wild comes in many formats including: plain text files, PDF, DOCX, html, json (twitter), etc.\n","\n","\n","The Python ecosystem is rich with packages that makes it easy to access the different types of files and APIs. "]},{"cell_type":"markdown","metadata":{"id":"aMx4wapt3LWq"},"source":["## Tokenization\n","\n","For analysis and further processing, we want to break up the string into words and punctuation. This step is called tokenization, and it produces our familiar structure, a list of words and punctuation.\n","\n","A sentence or data can be split into words using the method ```word_tokenize()```:"]},{"cell_type":"code","metadata":{"id":"86aP9BY2NtRl"},"source":["from nltk.tokenize import word_tokenize\n","\n","text = \"All work and no play makes jack a dull boy, all work and no play\"\n","word_tokens = word_tokenize(text)\n","print(word_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6e9TZtPnPd5t"},"source":["word_text = nltk.Text(word_tokens)\n","word_text.vocab().most_common(20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nx19Q3YUQ8tR"},"source":["The same principle can be applied to sentences. Simply change the to\n","```sent_tokenize()``` We have added two sentences to the variable data:"]},{"cell_type":"code","metadata":{"id":"ZKZkCb4LRNPD"},"source":["from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n","sent_tokenize(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"feC19DXnNata"},"source":["### Exercise 3\n","\n","What is the longest word in the provided article bellow? "]},{"cell_type":"code","metadata":{"id":"3rdejqn5L96A"},"source":["# We grab the article HTML\n","from urllib import request\n","\n","url = \"https://www.technologyreview.com/2021/03/17/1020811/better-tech-government-pandemic-united-states/\"\n","\n","response = request.urlopen(url)\n","print(response.code)\n","\n","raw = response.read().decode('utf8')\n","print(type(raw))\n","print(len(raw))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEfp-j3RT_zZ"},"source":["# Then we parse the response\n","from bs4 import BeautifulSoup\n","\n","parsed_text = BeautifulSoup(raw, 'html.parser').get_text()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6HEafw3oUwBn"},"source":["# your code goes here, Hint: tokenize > cast as Text object > analyse\n","\n","def find_longest_word(tokens):\n","  longest = ''\n","  for token in tokens:\n","    if len(token) > len(longest):\n","      longest = token\n","  return longest\n","\n","tokenized_text = word_tokenize(parsed_text[241122:253902])\n","\n","find_longest_word(tokenized_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W9X1M2J03TIr"},"source":["## Stop words\n","English text may contain stop words like ‘the’, ‘is’, ‘are’. Stop words can be filtered from the text to be processed. There is no universal list of stop words in NLP research, however the NLTK module contains a list of stop words. Now you will learn how to remove stop words using the NLTK. \n","\n","We start with the code from the previous section with tokenized words.\n","\n"]},{"cell_type":"code","metadata":{"id":"FIJ7tTTRNN0t"},"source":["from nltk.corpus import stopwords\n","print(stopwords.words('english'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jw4lYR7ESR6M"},"source":["from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords # We imported auxiliary corpus provided with NLTK\n","\n","text = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n","\n","stop_words = stopwords.words('english')\n","\n","words = word_tokenize(text.lower())\n","\n","words_filtered = [w for w in words if w not in stop_words]\n","\n","print(words_filtered) # Print the filtered text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AeM2rusPTSb3"},"source":["### Exercise 4\n","\n","using the variable from `parsed_text` from Exercise 3, find the most common 10 words before and after removing the stop words"]},{"cell_type":"code","metadata":{"id":"YRd03-nVTopA"},"source":["# Your code goes here\n","tokenized_text = word_tokenize(parsed_text[241122:253902])\n","tokenized_text = [word for word in tokenized_text if word.isalpha()]\n","\n","top10_words_before = FreqDist(tokenized_text).most_common(10)\n","print([word for word, freq in top10_words_before])\n","\n","tokenized_text_filtered = [word for word in tokenized_text if word not in stop_words]\n","top10_words_after = FreqDist(tokenized_text_filtered).most_common(10)\n","print([word for word, freq in top10_words_after])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0DYHrw3JZ9C"},"source":["## Stemming\n"]},{"cell_type":"markdown","metadata":{"id":"5rdQ0NUbYTOa"},"source":["Stemmers\n","\n","NLTK includes several off-the-shelf stemmers, and if you ever need a stemmer you should use one of these in preference to crafting your own using regular expressions, since these handle a wide range of irregular cases. The Porter and Lancaster stemmers follow their own rules for stripping affixes. \n","\n","Observe that the Porter stemmer correctly handles the word lying (mapping it to lie), while the Lancaster stemmer does not."]},{"cell_type":"code","metadata":{"id":"NuG8dzPAUEEB"},"source":["from nltk import PorterStemmer, LancasterStemmer\n","\n","porter = PorterStemmer()\n","\n","raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n"," is no basis for a system of government.  Supreme executive power derives from\n"," a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n","\n","tokens = word_tokenize(raw)\n","\n","[porter.stem(t) for t in tokens]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oqzIlLjoUXbU"},"source":["lancaster = LancasterStemmer()\n","\n","[lancaster.stem(t) for t in tokens]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dra2Jw0xUEz7"},"source":["Stemming is not a well-defined process, and we typically pick the stemmer that best suits the application we have in mind. \n","\n","**NOTE:**\n","Stemming is an important step for traditional statistical machine learning systems, however for deep learning based methods, stemming is not used.\n"]},{"cell_type":"markdown","metadata":{"id":"3hiV2YEaKeMX"},"source":["## Lemmatization"]},{"cell_type":"markdown","metadata":{"id":"1N-qEAvMYXNN"},"source":["The WordNet lemmatizer only removes affixes if the resulting word is in its dictionary. This additional checking process makes the lemmatizer slower than the above stemmers. Notice that it doesn't handle lying, but it converts women to woman."]},{"cell_type":"code","metadata":{"id":"SW-xDzkjWeyF"},"source":["wnl = nltk.WordNetLemmatizer()\n","print([wnl.lemmatize(t) for t in tokens])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0D_VMczyWh1h"},"source":["Compare the results of stemming and lemmatization."]},{"cell_type":"markdown","metadata":{"id":"9bn75rgXW6d4"},"source":["### Exercise 5"]},{"cell_type":"markdown","metadata":{"id":"0UUNd-k5KhQm"},"source":["Read the gutenberg Corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?"]},{"cell_type":"code","metadata":{"id":"HZ6iFD2mgkdB"},"source":["# You code goes here\n","# I picked bryant-stories.txt\n","wh_words = ['why', 'who', 'which', 'what', 'where', 'when', 'how']\n","tokens = gutenberg.words('bryant-stories.txt')\n","wh_tokens = [token for token in tokens if token.lower() in wh_words]\n","print(wh_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PeufdO9c2zHC"},"source":["# Spacy - NLP in production\n","\n","SpaCy is another open-source library for Natural Language Processing (NLP) in Python.\n","\n","spaCy is designed specifically for production (not for learning the concepts) use and helps you build applications that process and “understand” large volumes of text.\n","\n","The way tokenization is handled in Spacy is summerized in the following diagram:\n","\n","![](https://spacy.io/tokenization-9b27c0f6fe98dcb26239eba4d3ba1f3d.svg)"]},{"cell_type":"code","metadata":{"id":"nncZ9J_ZkWG2"},"source":["# upgrade spacy to the latest version\n","!pip install -U pip setuptools wheel\n","!pip install -U spacy\n","!python -m spacy download en_core_web_sm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJfwYlYdh4vI"},"source":["import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(\"Apple isn't looking at buying U.K. startup for $1 billion\")\n","print([token.text for token in doc])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-HgQvF-SkC93"},"source":["Lemmatization in Spacy"]},{"cell_type":"code","metadata":{"id":"orx3qdrbkBvA"},"source":["print([token.lemma_ for token in doc])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nta_Rb2rmzBS"},"source":["Notice the difference between the tokens and lemmas!"]},{"cell_type":"markdown","metadata":{"id":"bNAlp_Nuymjc"},"source":["# WordClouds\n","\n","Word clouds are a visualization technique which represent the frequency or the importance of each word. "]},{"cell_type":"code","metadata":{"id":"3ZHqvYY-ysDW"},"source":["!pip install wordcloud"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s6Qst4m5ywTh"},"source":["from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","from matplotlib import pyplot as plt\n","\n","\n","test_corpus2 = \" \".join(text1.tokens)\n","\n","\n","wordcloud = WordCloud(stopwords=STOPWORDS, background_color=\"white\").generate(test_corpus2)\n","\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iLhszjThlgDL"},"source":["### Exercise 6\n","\n","Generate a word cloud for the book at https://www.gutenberg.org/cache/epub/19017/pg19017.txt \n"]},{"cell_type":"code","metadata":{"id":"UV2lHjajl713"},"source":["# Your code goes here\n","import urllib.request  \n","import re\n","\n","# step 1: download the book as text\n","book_url = \"https://www.gutenberg.org/cache/epub/19017/pg19017.txt\"\n","\n","with urllib.request.urlopen(book_url) as f:\n","   book = f.read().decode('utf-8').lower()\n","\n","# step 2: tokenize and remove the irrelevant parts of the text\n","book = [word for word in word_tokenize(book) if word.isalpha()]\n","\n","\n","# step 3: generate and plot wordcloud \n","wordcloud = WordCloud(stopwords=STOPWORDS, background_color=\"white\")\n","wordcloud = wordcloud.generate(' '.join(book))\n","\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]}]}